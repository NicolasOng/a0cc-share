# Understanding Model vs Training Data Accuracy

## Definitions

$\pi(s)$: a probability distribution over the set of legal actions $A(s)$ for state $s$, where $\pi(a|s)$ is the probability of taking action $a$ at state $s$, and $\sum_{a \in A(s)} \pi(a|s) = 1$.

$v(s) \in [-1, 1]$: an estimated game outcome from the current player's perspective, where $+1$ represents a predicted win, $-1$ a predicted loss, and $0$ a predicted draw.

During self-play, we collect training examples consisting of:
- $s\in S_\text{sp}$: states encountered during games
- $\pi_{\text{sp}}(s)$: derived from MCTS visit count distributions
- $v_{\text{sp}}(s)$: the final game outcome (win = 1, loss = -1, draw = 0) for state $s$, from the perspective of the current player at $s$.
Note that $S_\text{sp}$ may include multiple equivalent states (e.g. $s_a=s_b$), and yet $\pi_{\text{sp}}(s_a)\neq\pi_{\text{sp}}(s_b)$ and $v_{\text{sp}}(s_a)\neq v_{\text{sp}}(s_b)$ are possible.

We have access to solve data:
- $\pi^*(s)$: the ground-truth best move distribution (can put the actual formula here)
- $v^*(s)$: the true value of state $s$ under optimal play

Value Accuracy definition:
$$\text{Acc}_v(\hat{v}, v^*, S) = \frac{1}{|S|} \sum_{s \in S} \mathbb{1}[\hat{v}(s) = v^*(s)]$$

$\mathbb{1}[\cdot]$ is the indicator function: equal to 1 if the inside is `true`, else 0.
$v(s)$ is usually assumed to be one of 1, -1.

Policy Accuracy definition:
$$\text{Acc}_\pi(\hat{\pi}, \pi^*, S) = \frac{1}{|S|} \sum_{s \in S} \mathbb{1}[\arg\max_{a \in A(s)} \hat{\pi}(a|s) \in A^*(s)]$$

where $A^*(s)$ is the set of optimal actions for state $s$ according to $\pi^*$ 

## Motivation
![](resources/2026-01-09%20merged_full_accuracy_value_ci.png)

![](resources/2026-01-12%20merged_full_accuracy_policy_ci.png)

We observe that the trained model achieves higher accuracy than the training data when both are evaluated against ground truth. How does the model surpass the accuracy of the data it is trained on?

## Hypotheses

### Noisy Training Data
The training data may have a very accurate value/policy function, its accuracy is just hurt by noise. By training on the data, the model may be able to extract the true underlying value/policy, which matches well with ground truth.

For example, a single state $s_1$ may be seen multiple times during self play, and maybe 60% of the time $v_\text{sp}(s_1)=v^*(s_1)$ and 40% of the time $v_\text{sp}(s_1)\neq v^*(s_1)$. Meaning the training data for $s_1$ is 60% accurate. However, a model trained on this data would likely always predict $v^*(s_1)$ for $s_1$, which is 100% accuracy.

### More Accurate Training Data Late Game
States that occur later in games may be more accurately evaluated by MCTS/self-play, meaning more accurate training data. Perhaps the model weighs these states accordingly, or is able to generalize out from them.

## Methodology
### Noisy Training Data

To determine the effect of noisy training data, we define hypothetical value and policy functions that represent the best possible performance the model can achieve on the training data. This is achieved through majority class selection.

Let $U = \{u_1, u_2, \ldots, u_M\}$ be the set of unique states in $S_{\text{sp}}$. For each unique state $u \in U$, define:
$$S_u = \{s \in S_{\text{sp}} : s = u\}$$

the set of all training examples corresponding to state $u$.

For each unique state $u \in U$, choose the value/policy that achieves the best possible performance on the training data:

$$v_{\text{BPP}}(u) = \arg\max_{v \in \{-1, 1\}} \text{Acc}_v(v, v_\text{sp}, S_u)$$

$$\pi_{\text{BPP}}(u) = \arg\max_{\pi \in \{\pi_{\text{sp}}(s) : s \in S_u\}} \text{Acc}_\pi(\pi, \pi_\text{sp}, S_u)$$

We can then measure the accuracy of this value/policy function against the training data to see what the best possible performance on the training data is.
$$
\text{Acc}_v(v_\text{BPP}, v_\text{sp}, S_\text{sp})
$$
$$
\text{Acc}_\pi(\pi_\text{BPP}, \pi_\text{sp}, S_\text{sp})
$$
Or we can measure this hypothetical model against the ground truth.
$$
\text{Acc}_v(v_\text{BPP}, v^*, S_\text{sp})
$$
$$
\text{Acc}_\pi(\pi_\text{BPP}, \pi^*, S_\text{sp})
$$
If this is higher or equal to the model we learn, then this would explain why the model performance is so high.

### More Accurate Training Data Late Game

To determine if the training data's accuracy increases further into games, we first define the "Game Progress" measure and bin states accordingly.

For each state $s$ encountered at turn $t$ in a game of length $T$, we define:
$$\text{GP}(s) = \frac{t}{T} \in [0, 1]$$

where $t = 0$ is the initial state and $t = T$ is the terminal state.

We partition $S_{\text{sp}}$ into $K = 10$ buckets based on game progress:
$$S_i = \{s \in S_{\text{sp}} : \frac{i-1}{K} \leq \text{GP}(s) < \frac{i}{K}\} \quad \text{for } i = 1, 2, \ldots, K$$

This creates buckets: $S_1$ (0-10%), $S_2$ (10-20%), ..., $S_{10}$ (90-100%).

Then for each bucket $S_i$, we compute:
   $$\text{Acc}_\pi(\pi_{\text{sp}}, \pi^*, S_i) \quad \text{and} \quad \text{Acc}_v(v_{\text{sp}}, \pi^*, S_i)$$

If we see higher accuracy than the model in any section of the game (most likely the end), that could explain where the model learns from.

## Results
### Noisy Training Data
BPP against training data results:

![](resources/2026-01-16%20bpp_series.png)

value BPP achieves ~97% accuracy on the training data. policy BPP achieves 85% - 90% (per iteration) and ~80% overall.

This shows that, among each state in the training data:
- there's on average a 97% agreement on the outcome experienced by the state.
- there's an average a 80% - 90% agreement on the action to take, as decided by MCTS.
Of course, the averages may be inflated by the many unique states (all with 100% agreement rate).

BPP against ground truth:

![](resources/2026-01-16%20bppma_series.png)

Conclusion: using BPP - essentially majority-class strategy - is able to increase the performance expected from the training data. However, it still doesn't explain the model's high performance, suggesting more mechanisms at play.
### More Accurate Training Data Late Game

![](resources/2026-02-10%20gp_merged_training_data_accuracy_over_game_progress_10.png)

Looks like later states (higher game progress) show significantly higher accuracy in the training data.

This suggests we may be on the right track. Next we should check if the model itself learns higher accuracy on later states. And if so, why does the overall model accuracy get higher than overall training accuracy? Does it generalize from these states? Weigh them more favourably?
